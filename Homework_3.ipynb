{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Student Name: Saransh Srivastava\n",
    "\n",
    "Student ID: 1031073"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>:  Friday, 17 May 2019 4pm\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 7% of mark for class (with 6% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib and Scikit-Learn. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>. \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models. Your objective is to create an automatic player which makes the fewest mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hangman Game (*No implementation is needed*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        This function plays the hangman game with the provided gusser and returns the number of incorrect guesses. \n",
    "        \n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a human guesser allowing interactive play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    This is a simple function for manual play.\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    return input().lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to play hangman interactively, please set `interactive` to True. When submitting your solution, set to False so we can automatically run the whole notebook using `Run All`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    hangman('whatever', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing Test Set and Training Set (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the algorithm. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown` and the `words` method, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40234\n",
      "1000\n",
      "39234\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# word_set stores all the unique word types in the Brown corpus\n",
    "word_set = []\n",
    "# test_set stores 1000 word types for testing\n",
    "test_set = []\n",
    "# training_set stores the rest word types for training\n",
    "training_set = []\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "count =0\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    if word.isalpha():\n",
    "        word_set.append(word)\n",
    "word_set = list(set(word_set))\n",
    "         \n",
    "np.random.shuffle(word_set)\n",
    "for word in word_set:        \n",
    "    if count < 1000:\n",
    "        test_set.append(word)\n",
    "    else:\n",
    "        training_set.append(word)\n",
    "    count +=1\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(word_set))\n",
    "print(len(test_set))\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(word_set) > 35000 and len(word_set) < 45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(test_set) == 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(training_set) + len(test_set) == len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Guesser: Random Guessing (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To help you measure the performance of this (and later) guesser, a `test_guesser` method that takes a guesser and measures the average number of incorrect guesses made over all the words in the `test_set` is provided to you. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_guesser(guesser, test=test_set):\n",
    "    \"\"\"\n",
    "        This function takes a guesser and measures the average number of incorrect guesses made over all the words in the test_set. \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for word in test:\n",
    "        total += hangman(word, guesser, 26, False)\n",
    "    return total / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  16.804\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "        This function implements a random guesser. It returns the random guess. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    while(True):\n",
    "        letter = np.random.choice(list(string.ascii_lowercase), size=1, replace=False)[0]\n",
    "        if letter not in guessed:\n",
    "            return letter\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "# uncomment to run a single hangman game with output shown (useful for debugging)\n",
    "#hangman(np.random.choice(test_set), random_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(random_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 10 and result < 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your First AI Guesser: Unigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guesser that returns the character with the highest probability. Remember to exclude already guessed characters. \n",
    "\n",
    "Hint: It should be much lower than random guessing.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 35158, 'i': 26059, 'a': 24363, 's': 23710, 'n': 22603, 'r': 22130, 't': 20639, 'o': 19158, 'l': 16650, 'c': 12597, 'd': 12275, 'u': 10191, 'm': 8684, 'g': 8611, 'p': 8552, 'h': 7351, 'b': 5737, 'y': 5346, 'f': 4204, 'v': 3438, 'k': 2987, 'w': 2804, 'z': 1025, 'x': 826, 'j': 644, 'q': 529})\n",
      "\n",
      "Average number of incorrect guesses:  10.326\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# unigram_counts stores the frequencies of characters over all training words\n",
    "unigram_counts = Counter()\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for word in training_set:\n",
    "    unigram_counts += Counter(list(word))\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(unigram_counts)\n",
    "\n",
    "def unigram_guesser(mask, guessed, unigram_counts=unigram_counts):\n",
    "    \"\"\"\n",
    "        This function implements a unigram guesser. It returns a guess based on the unigram model. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    count = 0\n",
    "    while(True):\n",
    "        letter = unigram_counts.most_common(count+1)[count][0]\n",
    "        count +=1\n",
    "        if letter not in guessed:\n",
    "            return letter\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "#hangman(np.random.choice(test_set), unigram_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your Second AI Guesser: Length-based Unigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. You should incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having a *different* unigram model for each length of the words. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. You need to decide on how to handle this situation.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "\n",
      "defaultdict(<class 'collections.Counter'>, {4: Counter({'e': 900, 'a': 866, 's': 698, 'o': 679, 'l': 557, 'i': 541, 'r': 514, 't': 489, 'n': 471, 'd': 363, 'u': 348, 'm': 296, 'p': 287, 'h': 286, 'c': 257, 'b': 256, 'k': 221, 'g': 221, 'w': 194, 'y': 176, 'f': 168, 'v': 97, 'j': 63, 'z': 45, 'x': 35, 'q': 8}), 7: Counter({'e': 5303, 'a': 3553, 's': 3459, 'r': 3423, 'i': 3341, 'n': 3040, 't': 2655, 'o': 2508, 'l': 2490, 'd': 2054, 'c': 1680, 'u': 1503, 'g': 1495, 'm': 1232, 'p': 1228, 'h': 1109, 'b': 944, 'y': 747, 'f': 667, 'k': 586, 'w': 520, 'v': 469, 'z': 153, 'x': 112, 'j': 97, 'q': 82}), 2: Counter({'a': 26, 'e': 20, 'm': 19, 'o': 18, 's': 18, 'i': 16, 'l': 13, 'h': 12, 'p': 12, 'u': 11, 't': 11, 'c': 11, 'd': 11, 'n': 10, 'f': 10, 'r': 9, 'b': 8, 'w': 7, 'v': 7, 'y': 6, 'j': 5, 'g': 5, 'k': 5, 'x': 3, 'q': 2, 'z': 1}), 10: Counter({'e': 4323, 'i': 3607, 'n': 3069, 's': 2891, 't': 2792, 'a': 2769, 'r': 2669, 'o': 2376, 'l': 1924, 'c': 1663, 'd': 1401, 'u': 1298, 'g': 1082, 'p': 1077, 'm': 1056, 'h': 821, 'b': 618, 'y': 541, 'f': 462, 'v': 445, 'w': 206, 'k': 178, 'z': 110, 'x': 91, 'q': 65, 'j': 56}), 6: Counter({'e': 4249, 'a': 2687, 's': 2613, 'r': 2536, 'i': 2048, 'n': 2004, 'o': 1981, 'l': 1954, 't': 1866, 'd': 1625, 'u': 1151, 'c': 1069, 'm': 940, 'g': 883, 'h': 870, 'p': 850, 'b': 772, 'y': 710, 'f': 497, 'k': 454, 'w': 432, 'v': 397, 'z': 124, 'j': 104, 'x': 97, 'q': 51}), 5: Counter({'e': 2194, 'a': 1832, 's': 1825, 'r': 1364, 'o': 1331, 'l': 1197, 'i': 1158, 't': 1079, 'n': 1028, 'd': 773, 'c': 702, 'u': 675, 'h': 594, 'm': 572, 'y': 545, 'p': 527, 'b': 501, 'g': 461, 'k': 378, 'f': 340, 'w': 304, 'v': 241, 'z': 88, 'j': 78, 'x': 63, 'q': 30}), 11: Counter({'e': 2955, 'i': 2750, 'n': 2274, 't': 2146, 'a': 2016, 's': 2002, 'r': 1922, 'o': 1701, 'l': 1321, 'c': 1225, 'u': 862, 'd': 805, 'p': 773, 'm': 739, 'g': 633, 'h': 522, 'b': 421, 'y': 406, 'f': 295, 'v': 286, 'k': 136, 'w': 122, 'x': 70, 'z': 69, 'q': 43, 'j': 27}), 8: Counter({'e': 5597, 'i': 3891, 'a': 3683, 's': 3675, 'n': 3506, 'r': 3502, 't': 2899, 'o': 2769, 'l': 2621, 'd': 2213, 'c': 1934, 'u': 1572, 'g': 1533, 'm': 1299, 'p': 1236, 'h': 1165, 'b': 855, 'y': 725, 'f': 707, 'v': 538, 'k': 517, 'w': 480, 'z': 110, 'x': 108, 'j': 85, 'q': 84}), 9: Counter({'e': 5424, 'i': 3952, 'n': 3462, 's': 3450, 'a': 3446, 'r': 3265, 't': 3063, 'o': 2612, 'l': 2322, 'c': 1908, 'd': 1871, 'u': 1439, 'g': 1373, 'm': 1248, 'p': 1153, 'h': 1020, 'b': 759, 'y': 652, 'f': 586, 'v': 530, 'w': 374, 'k': 374, 'z': 134, 'x': 121, 'q': 88, 'j': 68}), 12: Counter({'e': 1912, 'i': 1912, 'n': 1553, 't': 1484, 'a': 1427, 's': 1322, 'r': 1280, 'o': 1236, 'l': 962, 'c': 902, 'u': 592, 'p': 559, 'm': 521, 'd': 513, 'g': 397, 'h': 386, 'y': 298, 'b': 259, 'f': 200, 'v': 195, 'z': 70, 'w': 53, 'k': 52, 'x': 41, 'q': 37, 'j': 17}), 13: Counter({'i': 1280, 'e': 1039, 'n': 1030, 't': 968, 'a': 910, 'o': 848, 's': 794, 'r': 753, 'c': 573, 'l': 543, 'p': 362, 'm': 346, 'u': 334, 'd': 261, 'g': 247, 'h': 206, 'y': 204, 'b': 140, 'f': 117, 'v': 114, 'z': 37, 'x': 34, 'k': 27, 'w': 22, 'q': 20, 'j': 10}), 3: Counter({'a': 237, 'e': 194, 'o': 178, 'i': 129, 'n': 118, 's': 114, 't': 114, 'p': 104, 'm': 101, 'u': 100, 'd': 98, 'r': 97, 'b': 92, 'l': 80, 'h': 78, 'y': 76, 'g': 75, 'w': 72, 'c': 69, 'f': 51, 'k': 32, 'j': 28, 'v': 24, 'x': 22, 'z': 10, 'q': 5}), 16: Counter({'i': 159, 't': 123, 'a': 107, 'e': 103, 'o': 100, 'n': 100, 'r': 90, 'l': 87, 's': 82, 'c': 59, 'p': 44, 'h': 35, 'm': 34, 'y': 32, 'u': 29, 'd': 26, 'g': 22, 'f': 10, 'b': 10, 'z': 9, 'v': 6, 'x': 6, 'k': 4, 'q': 2, 'w': 1, 'j': 0}), 14: Counter({'i': 708, 'e': 571, 'n': 542, 't': 519, 's': 460, 'a': 452, 'o': 449, 'r': 395, 'l': 333, 'c': 301, 'p': 181, 'u': 170, 'm': 164, 'd': 150, 'h': 131, 'y': 117, 'g': 109, 'b': 70, 'v': 63, 'f': 58, 'z': 33, 'x': 13, 'k': 11, 'w': 10, 'q': 7, 'j': 3}), 17: Counter({'i': 91, 't': 79, 'e': 70, 'a': 67, 'n': 65, 'r': 62, 'o': 57, 's': 43, 'l': 39, 'c': 39, 'p': 27, 'm': 26, 'h': 26, 'd': 23, 'y': 18, 'u': 16, 'g': 12, 'z': 8, 'b': 5, 'v': 4, 'f': 2, 'j': 1, 'w': 1, 'x': 1, 'k': 0, 'q': 0}), 15: Counter({'i': 404, 't': 288, 'n': 280, 'o': 254, 'e': 251, 'a': 241, 's': 215, 'r': 202, 'l': 169, 'c': 169, 'p': 111, 'y': 79, 'm': 75, 'u': 74, 'd': 72, 'h': 71, 'g': 52, 'f': 26, 'b': 23, 'v': 20, 'z': 18, 'x': 8, 'k': 8, 'w': 5, 'q': 4, 'j': 1}), 18: Counter({'i': 48, 't': 44, 'e': 37, 's': 35, 'r': 34, 'o': 33, 'n': 29, 'a': 25, 'c': 24, 'l': 23, 'p': 14, 'h': 13, 'd': 12, 'y': 10, 'm': 9, 'u': 8, 'g': 6, 'f': 4, 'b': 2, 'k': 2, 'z': 1, 'v': 1, 'j': 0, 'q': 0, 'w': 0, 'x': 0}), 1: Counter({'g': 1, 'v': 1, 'u': 1, 'h': 1, 'p': 1, 'z': 1, 'b': 1, 'k': 1, 'a': 1, 'e': 1, 'l': 1, 'w': 1, 's': 1, 't': 1, 'o': 1, 'm': 1, 'x': 1, 'j': 1, 'i': 1, 'r': 1, 'y': 1, 'n': 1, 'q': 1, 'd': 1, 'c': 1, 'f': 1}), 20: Counter({'i': 5, 't': 4, 'n': 3, 'o': 2, 'a': 2, 's': 1, 'u': 1, 'l': 1, 'z': 1, 'b': 0, 'c': 0, 'd': 0, 'e': 0, 'f': 0, 'g': 0, 'h': 0, 'j': 0, 'k': 0, 'm': 0, 'p': 0, 'q': 0, 'r': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0}), 19: Counter({'o': 18, 'i': 15, 'n': 14, 't': 13, 'a': 11, 'r': 9, 'e': 8, 's': 7, 'l': 7, 'c': 7, 'u': 5, 'm': 3, 'p': 3, 'g': 3, 'd': 3, 'f': 2, 'z': 2, 'h': 2, 'y': 1, 'b': 0, 'j': 0, 'k': 0, 'q': 0, 'v': 0, 'w': 0, 'x': 0}), 21: Counter({'o': 6, 'c': 4, 'i': 3, 'm': 3, 'e': 3, 'l': 3, 'r': 3, 'p': 3, 'h': 3, 's': 3, 'a': 3, 'u': 1, 'n': 1, 't': 1, 'y': 1, 'g': 1, 'b': 0, 'd': 0, 'f': 0, 'j': 0, 'k': 0, 'q': 0, 'v': 0, 'w': 0, 'x': 0, 'z': 0}), 22: Counter({'e': 4, 'l': 3, 'n': 3, 'a': 2, 's': 2, 'k': 1, 'y': 1, 'b': 1, 'z': 1, 'u': 1, 'f': 1, 'o': 1, 't': 1, 'c': 0, 'd': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'm': 0, 'p': 0, 'q': 0, 'r': 0, 'v': 0, 'w': 0, 'x': 0})})\n",
      "\n",
      "Average number of incorrect guesses:  10.28\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# unigram_counts_by_length stores a dictionary, mapping word length to the frequencies of characters of words with that word length\n",
    "unigram_counts_by_length = defaultdict(Counter)\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for word in training_set:\n",
    "    unigram_counts_by_length[len(word)] += (Counter(list(word)))\n",
    "\n",
    "for key, value in unigram_counts_by_length.items():\n",
    "    for letter in list(string.ascii_lowercase):\n",
    "        if letter not in value.keys():\n",
    "            unigram_counts_by_length[key][letter] = 0\n",
    "    \n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "lengths = sorted(unigram_counts_by_length.keys())\n",
    "max_length = lengths[-1] + 1\n",
    "print(lengths)\n",
    "print()\n",
    "print(unigram_counts_by_length)\n",
    "\n",
    "def unigram_length_guesser(mask, guessed, counts=unigram_counts_by_length):\n",
    "    \"\"\"\n",
    "        This function implements a length-based unigram guesser. It returns a guess based on the length-based unigram model. \n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    count = 0\n",
    "    mask_len = len(mask)\n",
    "    while(True):\n",
    "        if mask_len > max_length:\n",
    "            mask_len = max_length\n",
    "        letter = unigram_counts_by_length[mask_len].most_common(count+1)[count][0]\n",
    "        count +=1\n",
    "        if letter not in guessed:\n",
    "            return letter\n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "#hangman(np.random.choice(test_set), unigram_length_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_length_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result > 5 and result < 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your Third AI Guesser: Bigram Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Now for the next challenge, using a *bigram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "You should develop a *bigram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with a sentinel symbol `$`.) You should use *linear interpolation* to smooth between the higher order and lower order models, and you will have to decide how to weight each component (be reminded that all probabilities should sum to 1).\n",
    "\n",
    "Your bigram guesser should apply your language model to each blank position in the secret word by using the left context as is known. E.g., in the partial word `$ _ e c _ e _ _` we know the left context for the first three blanks, but have no known left context for the last blank. Using a bigram language model, we are able to apply it to the first three blanks only. You should then select the character with the highest probability of predicting the most number of correct entries over the blanks. \n",
    "\n",
    "Do you see any improvement over the unigram guessers above?\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  10.064\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from numpy.random import choice \n",
    "import random\n",
    "import operator\n",
    "\n",
    "bigram_counts = None # you will want a different data structure to store the bigram \n",
    "\n",
    "def convert_sentence(word):\n",
    "    result = [\"<s>\"] + [w.lower() for w in word]\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_counts(sentences):\n",
    "    bigram_counts = defaultdict(Counter)\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence(sentence)\n",
    "        bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "        for bigram in bigram_list:\n",
    "            first, second = bigram\n",
    "            bigram_counts[first][second] += 1\n",
    "    return bigram_counts\n",
    "\n",
    "bigram_counts = get_counts(training_set)\n",
    "\n",
    "for key, value in bigram_counts.items():\n",
    "    for letter in list(string.ascii_lowercase):\n",
    "        if letter not in value.keys():\n",
    "            bigram_counts[key][letter] = 0\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "def bigram_guesser(mask, guessed, counts=bigram_counts): # add extra default arguments if needed\n",
    "    \"\"\"\n",
    "        This function implements a bigram guesser. It returns a guess based on the bigram model using linear interpolation.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    current_list = [\"<s>\"]\n",
    "    for idx,blank in enumerate(mask):\n",
    "        if blank != '_' and idx != len(mask):\n",
    "            current_list.append(blank)\n",
    "    mask = convert_sentence(mask)\n",
    "    result = {}\n",
    "    bigram_candidate = Counter()\n",
    "    \n",
    "    for item in set(current_list):\n",
    "        bigram_candidate = bigram_candidate + bigram_counts[item]\n",
    "\n",
    "    count3 = 0\n",
    "    while(True):\n",
    "        letter = bigram_candidate.most_common(count3+1)[count3][0]\n",
    "        count3 +=1\n",
    "        if letter not in guessed:\n",
    "            return letter \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "#hangman(np.random.choice(test_set), bigram_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(bigram_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(result < 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Your Own AI Guesser (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** You should try to develop a more effective AI, `my_ai_guesser`, for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word, fancier smoothing methods, and using ngram models. Ensure you report the test performance of your guesser. Have fun!\n",
    "\n",
    "You will be marked based on the explanation of your approach and its accuracy. \n",
    "\n",
    "(1 mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of incorrect guesses:  9.199\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "def my_ai_guesser(mask, guessed):\n",
    "    current_list = [\"<s>\"]\n",
    "    for idx,blank in enumerate(mask):\n",
    "        if blank != '_' and idx != len(mask):\n",
    "            current_list.append(blank)\n",
    "    mask = convert_sentence(mask)\n",
    "    \n",
    "    result = {}\n",
    "    for item in set(current_list):\n",
    "        indices = [i for i, x in enumerate(current_list) if x == item]\n",
    "        for index in indices:\n",
    "            if index < len(mask)-1 and mask[index+1] == '_':\n",
    "                prev_word_counts = bigram_counts[item]\n",
    "                total_counts = float(sum(prev_word_counts.values()))\n",
    "                count = 0\n",
    "                letter = prev_word_counts.most_common(count+1)[count][0]\n",
    "                letter_val = prev_word_counts.most_common(count+1)[count][1]\n",
    "                while letter in guessed:\n",
    "                    count += 1\n",
    "                    letter = prev_word_counts.most_common(count+1)[count][0]\n",
    "                    letter_val = prev_word_counts.most_common(count+1)[count][1]\n",
    "                result[letter] = letter_val/total_counts\n",
    "    return max(result.items(), key=operator.itemgetter(1))[0]        \n",
    "    \n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "result = test_guesser(my_ai_guesser)\n",
    "print()\n",
    "print(\"Average number of incorrect guesses: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:** Explain your approach and discuss your result below. Please keep it brief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my guesser, instead of taking the sum of distributions of left context key, I have compared the probability of occurance of a character after the left context. Based on whichever probability is higher, that becomes the next guess.\n",
    "The approach reduces the incorrect guess more than the bigram vanilla model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
