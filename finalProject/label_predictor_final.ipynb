{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Prediction\n",
    "Code to predict based on every claim-evidence as a pair and then finding max of label assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "wiki_path = 'wiki-pages-text/*.txt'\n",
    "test_path = 'testoutput-unlabelled.json'\n",
    "\n",
    "LOGISTIC_MODEL = \"logRegModel.sav\"\n",
    "VECTORIZER = \"Vectorizer.sav\"\n",
    "ENCODER = \"Encoder.sav\"\n",
    "ANN_MODEL = \"SequentialANN.sav\"\n",
    "\n",
    "LOGIST_PREDICTION = \"testoutput_baseline.json\"\n",
    "ANN_PREDICTION = \"testoutput.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDictionary(wiki_path):\n",
    "    files = glob.glob(wiki_path)\n",
    "    dict_doc = {}\n",
    "    for fname in files:\n",
    "        with open(fname) as f:\n",
    "            for text in f:\n",
    "                line = text.split()\n",
    "                pid = unicodedata.normalize('NFD',line[0])\n",
    "                sid = str(line[1])\n",
    "                sent = \" \".join(line[2:])\n",
    "                dict_doc[(pid,sid)] = sent\n",
    "\n",
    "    print(len(dict_doc.keys()))\n",
    "    return dict_doc\n",
    "\n",
    "    \n",
    "def loadModel(filename):\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    return loaded_model\n",
    "\n",
    "############\n",
    "### Reading test data with evidence\n",
    "############\n",
    "\n",
    "def loadTestDataDF(dict_doc,test_path):\n",
    "    with open(test_path) as f:\n",
    "        jTestdata = json.load(f)\n",
    "\n",
    "    df_list = []\n",
    "    for source in jTestdata.items():\n",
    "        texts = []\n",
    "        id = source[0]\n",
    "        values = source[1]\n",
    "        evidence = values['evidence']\n",
    "        claim = values['claim']\n",
    "        for evi in evidence:\n",
    "            pid = unicodedata.normalize('NFD',evi[0])\n",
    "            sid = str(evi[1])\n",
    "            df_list.append((id, claim ,(pid,sid), dict_doc[(pid,sid)]))\n",
    "\n",
    "        if len(evidence) == 0:\n",
    "            df_list.append((id,claim,(\"\",0),\"\"))\n",
    "\n",
    "    df = pd.DataFrame(df_list,columns=['id','claim','evidenceID','evidence'])\n",
    "    return df\n",
    "\n",
    "    \n",
    "def mergeDFs(dataDF,modelDF):\n",
    "    comb_df = dataDF.join(modelDF).drop(['claim_evidence','evidence'],axis=1)\n",
    "    grouped_df = comb_df.groupby(['id','label']).count()\n",
    "    grouped_max_df = grouped_df.loc[grouped_df.groupby([\"id\"])[\"claim\"].idxmax()] \n",
    "    merge_df = pd.merge(comb_df,\n",
    "                       grouped_max_df,on=['id','label'],how='inner')\n",
    "    merge2_df = merge_df.groupby(['id']).head(5)\n",
    "    testlist = merge2_df.groupby(['id','claim_x','label'])['evidenceID_x'].apply(list)\n",
    "    final_df = pd.DataFrame(testlist)\n",
    "    return final_df\n",
    "\n",
    "def storeDFasJSON(dataDF,filename):\n",
    "    document = {}\n",
    "    for index, row in dataDF.iterrows():\n",
    "        element = {}\n",
    "        element['claim'] = index[1]\n",
    "        element['label'] = index[2]\n",
    "        element['evidence'] = list(row)[0]\n",
    "        if index[2] == 'NOT ENOUGH INFO':\n",
    "            element['evidence'] = []\n",
    "        document[index[0]] = element\n",
    "    json.dump(document, open(filename, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load wiki data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25248313\n"
     ]
    }
   ],
   "source": [
    "dict_doc = getDictionary(wiki_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression predictor\n",
    "This method predicts on test data based on stored logistic regression trained over the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18780, 46358)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>evidenceID_x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>claim_x</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100038</th>\n",
       "      <th>Ripon College's student number totaled in at around 840.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(Ripon_College_-LRB-Wisconsin-RRB-, 1), (List...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100083</th>\n",
       "      <th>Kesha was baptized on March 1st, 1987.</th>\n",
       "      <th>NOT ENOUGH INFO</th>\n",
       "      <td>[(March_2007_lunar_eclipse, 7)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100169</th>\n",
       "      <th>Birthday Song (2 Chainz song) was banned by Sonny Digital.</th>\n",
       "      <th>NOT ENOUGH INFO</th>\n",
       "      <td>[(, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100234</th>\n",
       "      <th>The University of Illinois at Chicago is a college.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(The_University_of_Illinois_vs._a_Mummy, 0), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100359</th>\n",
       "      <th>French Indochina was officially known as the Indochinese Union in England after 1887.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(French_Indochinese_piastre, 0), (French_Indo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                evidenceID_x\n",
       "id     claim_x                                            label                                                             \n",
       "100038 Ripon College's student number totaled in at ar... SUPPORTS         [(Ripon_College_-LRB-Wisconsin-RRB-, 1), (List...\n",
       "100083 Kesha was baptized on March 1st, 1987.             NOT ENOUGH INFO                    [(March_2007_lunar_eclipse, 7)]\n",
       "100169 Birthday Song (2 Chainz song) was banned by Son... NOT ENOUGH INFO                                            [(, 0)]\n",
       "100234 The University of Illinois at Chicago is a coll... SUPPORTS         [(The_University_of_Illinois_vs._a_Mummy, 0), ...\n",
       "100359 French Indochina was officially known as the In... SUPPORTS         [(French_Indochinese_piastre, 0), (French_Indo..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "### Logistic regression prediction on the claium-evidence pair\n",
    "############\n",
    "\n",
    "def predictLogisticRegression(vectorizer,encoder,sentences):\n",
    "    classifier = loadModel(LOGISTIC_MODEL)\n",
    "    test_sentences  = vectorizer.transform(sentences)\n",
    "    print(test_sentences.shape)\n",
    "    test_labels = classifier.predict(test_sentences)\n",
    "    final_test_label = encoder.inverse_transform(test_labels)\n",
    "    return(final_test_label)\n",
    "\n",
    "testdata_df = loadTestDataDF(dict_doc,test_path)\n",
    "vectorizer = loadModel(VECTORIZER)\n",
    "encoder = loadModel(ENCODER)\n",
    "sentences = [sent for sent in testdata_df['claim'] + testdata_df['evidence']]\n",
    "final_test_label = predictLogisticRegression(vectorizer,encoder,sentences)\n",
    "\n",
    "logistic_result = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    logistic_result.append((final_test_label[(idx)],sent))\n",
    "\n",
    "logistic_df = pd.DataFrame(logistic_result,columns=['label','claim_evidence'])\n",
    "final_logistic_df = mergeDFs(testdata_df,logistic_df)\n",
    "storeDFasJSON(final_logistic_df,LOGIST_PREDICTION)\n",
    "final_logistic_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential ANN\n",
    "prediction on testing data based on trained and stored ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>evidenceID_x</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>claim_x</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100030</th>\n",
       "      <th>Steve Wozniak designed homes.</th>\n",
       "      <th>NOT ENOUGH INFO</th>\n",
       "      <td>[(, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100046</th>\n",
       "      <th>The Guthrie Theater's second building began operating in 1963.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(Theatre_building,_Zrenjanin, 0), (Guthrie_Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100060</th>\n",
       "      <th>Kareena Kapoor was initially successful.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(Kareena_Kapoor, 6), (Kareena_Kapoor, 1), (Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100088</th>\n",
       "      <th>Men in Black II stars Patrick Stewart.</th>\n",
       "      <th>NOT ENOUGH INFO</th>\n",
       "      <td>[(Patrick_Stewart, 6), (II_-LRB-Boyz_II_Men_al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100138</th>\n",
       "      <th>Brazilian jiu-jitsu is a form of armed combat.</th>\n",
       "      <th>SUPPORTS</th>\n",
       "      <td>[(Blake_canonical_form, 1), (Boyce–Codd_normal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                evidenceID_x\n",
       "id     claim_x                                            label                                                             \n",
       "100030 Steve Wozniak designed homes.                      NOT ENOUGH INFO                                            [(, 0)]\n",
       "100046 The Guthrie Theater's second building began ope... SUPPORTS         [(Theatre_building,_Zrenjanin, 0), (Guthrie_Th...\n",
       "100060 Kareena Kapoor was initially successful.           SUPPORTS         [(Kareena_Kapoor, 6), (Kareena_Kapoor, 1), (Ka...\n",
       "100088 Men in Black II stars Patrick Stewart.             NOT ENOUGH INFO  [(Patrick_Stewart, 6), (II_-LRB-Boyz_II_Men_al...\n",
       "100138 Brazilian jiu-jitsu is a form of armed combat.     SUPPORTS         [(Blake_canonical_form, 1), (Boyce–Codd_normal..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "### Predicting: Sequential ANN prediction on the evidence-claium pair\n",
    "############\n",
    "\n",
    "def predictANNSequential(vectorizer,encoder,sentences):\n",
    "    model = loadModel(ANN_MODEL)\n",
    "    vectorizer = loadModel(VECTORIZER)\n",
    "    encoder = loadModel(ENCODER)\n",
    "    test_sentences  = vectorizer.transform(sentences)\n",
    "    test_labels = model.predict_classes(test_sentences)\n",
    "    final_test_label = encoder.inverse_transform(test_labels)\n",
    "    return final_test_label\n",
    "\n",
    "\n",
    "testdata_df = loadTestDataDF(dict_doc,test_path)\n",
    "sentences = [sent for sent in testdata_df['claim'] + testdata_df['evidence']]\n",
    "vectorizer = loadModel(VECTORIZER)\n",
    "encoder = loadModel(ENCODER)\n",
    "final_test_label = predictANNSequential(vectorizer,encoder,sentences)\n",
    "\n",
    "sequential_result = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    sequential_result.append((final_test_label[(idx)],sent))\n",
    "    \n",
    "sequential_df = pd.DataFrame(sequential_result,columns=['label','claim_evidence'])\n",
    "final_ann_df = mergeDFs(testdata_df,sequential_df)\n",
    "storeDFasJSON(final_ann_df,ANN_PREDICTION)\n",
    "final_ann_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ann_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
