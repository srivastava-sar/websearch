{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Author Name: Saransh Srivastava \n",
    "\n",
    "Student ID: 1031073\n",
    "\n",
    "We test our evidences based on two models. A base line logistic regression and a sequential artifitial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "wiki_path = 'wiki-pages-text/*.txt'\n",
    "train_path = 'train.json'\n",
    "\n",
    "LOGISTIC_MODEL = \"logRegModel.sav\"\n",
    "VECTORIZER = \"Vectorizer.sav\"\n",
    "ENCODER = \"Encoder.sav\"\n",
    "ANN_MODEL = \"SequentialANN.sav\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeModel(model,filename):\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read wiki data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(wiki_path)\n",
    "dict_doc = {}\n",
    "for fname in files:\n",
    "    with open(fname) as f:\n",
    "        for text in f:\n",
    "            line = text.split()\n",
    "            pid = unicodedata.normalize('NFD',line[0])\n",
    "            sid = line[1]\n",
    "            sent = \" \".join(line[2:])\n",
    "            dict_doc[(pid,sid)] = sent\n",
    "\n",
    "print(len(dict_doc.keys()))\n",
    "with open(train_path) as f:\n",
    "    jdata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building training data for training of all the models\n",
    "\n",
    "This needs to run once, it saves the model in a file after that which can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### Building simple baseline logistic regression model\n",
    "##############\n",
    "\n",
    "###############\n",
    "### Method : make duplicate claims andlabel each claim-evidence pair\n",
    "##  Merge them into one output per claim with list of most dominant evidence\n",
    "###############\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for source in jdata.items():\n",
    "    texts = []\n",
    "    document = {}\n",
    "    values = source[1]\n",
    "    document['claim'] = values['claim']\n",
    "    label = values['label']\n",
    "    evidence = values['evidence']\n",
    "    if label == 'SUPPORTS' or label == 'REFUTES':\n",
    "        for evi in evidence:\n",
    "            pid = unicodedata.normalize('NFD',evi[0])\n",
    "            sid = str(evi[1])\n",
    "            if (pid,sid) in dict_doc:\n",
    "                document['claim'] += dict_doc[(pid,sid)]\n",
    "                document['label'] = label\n",
    "                df_list.append(document)\n",
    "                document = {}\n",
    "                document['claim'] = values['claim']\n",
    "    else:\n",
    "        document['label'] = label\n",
    "        df_list.append(document)\n",
    "        document = {}\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "print(df.label.unique())\n",
    "\n",
    "sentences = [sent for sent in df['claim']]\n",
    "# SUPPORTS = 2\"\n",
    "# REFUTES = 1\n",
    "# NOT ENOUGH INFO = 0\n",
    "labels = [sent for sent in df['label']]\n",
    "y = []\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.1, random_state=1000)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial',max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "############\n",
    "### Storing above logistic regression model\n",
    "############\n",
    "\n",
    "storeModel(classifier,LOGISTIC_MODEL)\n",
    "storeModel(vectorizer,VECTORIZER)\n",
    "storeModel(encoder,ENCODER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Sequential ANN model\n",
    "This needs to be trained once, it will then be stored and used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Sequencia ANN\n",
    "###############\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "model = Sequential()\n",
    "model.add(Dense(512,input_shape=(input_dim, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fit the model using the training data and validate using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12340\n",
    "epochs = 7\n",
    "history = model.fit(X_train,y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test))\n",
    "\n",
    "storeModel(vectorizer,VECTORIZER)\n",
    "storeModel(encoder,ENCODER)\n",
    "storeModel(model,ANN_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss vs Accuracy plot\n",
    "\n",
    "This plot shows us that we have reached our minimum loss and maximum accuracy without overfitting our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
