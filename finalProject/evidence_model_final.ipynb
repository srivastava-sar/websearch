{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Fact Verification system \n",
    "\n",
    "Author Name: Saransh Srivastava \n",
    "\n",
    "Student ID: 1031073\n",
    "\n",
    "### Document retrieval\n",
    "Read documents into memory as inverted index matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "##### Read all documents and make an inverted index\n",
    "#############\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import glob\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "path = 'wiki-pages-text/*.txt'\n",
    "files = glob.glob(path)\n",
    "inv_indx = defaultdict(Counter)\n",
    "dict_doc_all = {}\n",
    "for fname in files:\n",
    "    file_name = fname.split('/')[-1]\n",
    "    with open(fname) as f:\n",
    "        for idx,text in enumerate(f):\n",
    "            line = text.split()\n",
    "            text = line[2:]\n",
    "            pid_raw = unicodedata.normalize('NFD',line[0])\n",
    "            for pid in pid_raw.split('_'):\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "                text.append(pid)\n",
    "            for word in text:\n",
    "                inv_indx[word.lower()][fname] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to compute sentence similarity  using Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # sentence similarity computation using Wordnet\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        try:\n",
    "            best_score = ([synset.path_similarity(ss) for ss in synsets2])\n",
    "            best_score = [x for x in best_score if x is not None]\n",
    "            if best_score:\n",
    "                best_score = max(best_score)\n",
    "\n",
    "\n",
    "            # Check that the similarity could have been computed\n",
    "            if best_score:\n",
    "                score += best_score\n",
    "                count += 1\n",
    "        except WordNetError:\n",
    "            print(\"WordNetError detected!!\")\n",
    "            pass\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    # Average the values\n",
    "    if count !=0:\n",
    "        score /= count\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_similarity(sentence1, sentence2):\n",
    "    return (sentence_similarity(sentence1, sentence2) + sentence_similarity(sentence2, sentence1)) / 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to compute jacob similarity\n",
    "We use this similarity to order top 10 retrieved sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_jacob_similarity(sentence1,sentence2):\n",
    "    a = set(i.lower() for i in sentence1.split()) \n",
    "    b = set(j.lower() for j in sentence2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))    \n",
    "\n",
    "def symmetric_sentence_jacob_similarity(sentence1,sentence2):\n",
    "    return (sentence_jacob_similarity(sentence1, sentence2) + sentence_jacob_similarity(sentence2, sentence1)) / 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to get top N sentences from top K documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#### get top N sentences from the doc PIDS stored in memory\n",
    "###############\n",
    "\n",
    "def getSentence(dict_doc,id_list):\n",
    "    #############\n",
    "    ### get all sentence text from pid,sid list\n",
    "    ############\n",
    "    result = []\n",
    "    for ids in id_list:\n",
    "        result.append(dict_doc[(ids[0],str(ids[1]))])\n",
    "    return result\n",
    "\n",
    "def pairwise(myList):\n",
    "    tuples = [x+'_'+y for x,y in zip(myList, myList[1:])]\n",
    "    return tuples\n",
    "\n",
    "def getPriomaryTopicList(topic):\n",
    "    ##########\n",
    "    ## Preference order:\n",
    "    ## 1. pair-wise PNoun > Pnoun \n",
    "    ## 2. In order of appearance in the sentence\n",
    "    ##########\n",
    "    \n",
    "    nouns = []\n",
    "    pnoun = []\n",
    "    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(topic))):\n",
    "        #print(word,pos)\n",
    "        if (pos == 'NNP' or pos == 'NNPS'):\n",
    "            pnoun.append(word)      \n",
    "        if (pos == 'NN' or pos == 'NNS' or pos == 'NNPS'):\n",
    "             nouns.append(word)\n",
    "    properN = \"\"\n",
    "    couples = pairwise(pnoun)\n",
    "    pnoun = couples + pnoun\n",
    "    pnoun += nouns\n",
    "    return pnoun\n",
    "\n",
    "def doesExists(word, word_dict):\n",
    "    dict_keys = [key.split('_') for key in word_dict.keys()]\n",
    "    if dict_keys:\n",
    "        if word in dict_keys[0]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def getTopNSentIDFromPID(topics,k):\n",
    "    ##############\n",
    "    #### Make a dict of topic: retrieved sentences\n",
    "    #### if a new topic is a subset of the values in dict.keys() (eg: Bosnan exists in Paul_Bosnan)\n",
    "    #### then skip 'Bosnan': else look for sentences which has pid 'Bosnan'\n",
    "    ##############\n",
    "    dict_docL = getTopicEvidenceSentences(topics) # return relevant dict of (pid,sent): sentence\n",
    "    result = []\n",
    "    topics = removeStop(topics)\n",
    "    topic_dict = {}\n",
    "    topics = getPriomaryTopicList(topics)\n",
    "    for topic in topics:\n",
    "        if not doesExists(topic,topic_dict):\n",
    "            for (pid,sid) in dict_docL.keys():\n",
    "                if topic in pid:\n",
    "                    topic_dict[topic] = 1\n",
    "                    result.append((pid,sid))\n",
    "                    if len(result) >= k:\n",
    "                        return dict_docL,result\n",
    "        \n",
    "    return dict_docL,result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to get top K documents from wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#### get top K documents for a query\n",
    "##################\n",
    "\n",
    "def getTopKDoc(topic , k):\n",
    "    inv_idx = inv_indx\n",
    "    can_doc = Counter()\n",
    "    for word in topic.split():\n",
    "        can_doc += inv_idx[word.lower()]\n",
    "    res_list = [x[0] for x in can_doc.most_common(k)]\n",
    "    return res_list\n",
    "\n",
    "def removeStop(raw_words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokenized_words = raw_words.split()\n",
    "    separator = \" \"\n",
    "    return separator.join([word for word in tokenized_words if word not in stop_words])\n",
    "    \n",
    "def getKeyWords(topic_raw):\n",
    "    sentence = nltk.pos_tag(topic_raw.split())\n",
    "    csent = nltk.ne_chunk(sentence)\n",
    "    iob_tagged = tree2conlltags(csent)\n",
    "    result = []\n",
    "    for w,t,c in iob_tagged:\n",
    "        if c != 'O':\n",
    "            result.append(w)\n",
    "    return(' '.join(result))\n",
    "    \n",
    "    \n",
    "def getRealTopic(topic_raw):\n",
    "    topic = removeStop(topic_raw)\n",
    "    return topic\n",
    "\n",
    "\n",
    "def getTopicEvidenceSentences(topic_raw):\n",
    "    topic = getRealTopic(topic_raw)\n",
    "    topKDoc = getTopKDoc(topic,7)\n",
    "    for top in topic.split():\n",
    "        for docum in getTopKDoc(top,1):\n",
    "            if docum not in topKDoc:\n",
    "                topKDoc.append(docum)\n",
    "    dict_doc = {}\n",
    "    for fname in topKDoc:\n",
    "        file_name = fname.split('/')[-1]\n",
    "        with open(fname) as f:\n",
    "            for text in f:\n",
    "                line = text.split()\n",
    "                pid = unicodedata.normalize('NFD',line[0])\n",
    "                sid = line[1]\n",
    "                pid_split = pid.split('_')\n",
    "                sent_raw = pid_split + line[2:]\n",
    "                sent = \" \".join(sent_raw)\n",
    "                dict_doc[(pid,sid)] = sent\n",
    "    return dict_doc\n",
    "\n",
    "def getBestDoc(query_text,k):\n",
    "    dict_doc,textsIds = getTopNSentIDFromPID(query_text,k)\n",
    "    return dict_doc,textsIds\n",
    "\n",
    "def tryQuery(query_text):\n",
    "    query_text = getRealTopic(query_text)\n",
    "    dict_doc,result_documentsIds = getBestDoc(query_text,500)\n",
    "    return dict_doc,result_documentsIds\n",
    "\n",
    "def storeData(dataObj,filename):\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(dataObj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to get most(=size) similar sentences for a query (claim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvidenceID(claim_text,size):\n",
    "    dict_doc,res_ID = tryQuery(claim_text)\n",
    "    focus_sentence = claim_text\n",
    "    sentences = getSentence(dict_doc,res_ID)\n",
    "    result = []\n",
    "    for idx,sentence in enumerate(sentences):\n",
    "        score1 = symmetric_sentence_similarity(focus_sentence, sentence)\n",
    "        score2 = symmetric_sentence_jacob_similarity(focus_sentence, sentence)\n",
    "        if score1 > 0.4 :\n",
    "            result.append((score1,score2, res_ID[idx],sentence))\n",
    "    result = list(set(result))\n",
    "    result.sort(reverse=True)\n",
    "    return result[:size]\n",
    "\n",
    "def getEvidenceSent(evidences_list):\n",
    "    result = []\n",
    "    for evidence in evidences_list:\n",
    "        if evidence[1]>0.09:\n",
    "            result.append((evidence[1],evidence[0],evidence[2],evidence[3]))\n",
    "    result.sort(reverse=True)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def getEvidence(source):\n",
    "    df_Testlist = {}\n",
    "    texts = []\n",
    "    document = {}\n",
    "    id = str(source[0])\n",
    "    values = source[1]\n",
    "    dict_value = {}\n",
    "    dict_value['claim'] = values['claim']\n",
    "    evidences = getEvidenceID(dict_value['claim'],10)\n",
    "    evidences2 = getEvidenceSent(evidences)\n",
    "    evid_list = []\n",
    "    for evidence in evidences2:\n",
    "        evid_list.append(list(evidence[2]))\n",
    "        \n",
    "    dict_value[\"evidence\"] = evid_list\n",
    "    document[id] = dict_value\n",
    "    df_Testlist = {**df_Testlist, **document} \n",
    "    \n",
    "    return df_Testlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data and extracts best sentences matching it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### Parallelizing: Read test data and get 'best' evidecnces\n",
    "##################\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import ChainMap\n",
    "\n",
    "test_path = 'test-unlabelled.json'\n",
    "\n",
    "cpu_count = mp.cpu_count() - 4\n",
    "print(\"CPU count: \" + str(cpu_count))\n",
    "pool = mp.Pool(cpu_count)\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(test_path) as f:\n",
    "    jTestdata = json.load(f)\n",
    "\n",
    "t3 = time.time()\n",
    "\n",
    "result_objects = [pool.apply_async(getEvidence,\n",
    "                                   args=(row,)) for row in jTestdata.items()]\n",
    "\n",
    "results = [r.get() for r in result_objects]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "final_result = dict(ChainMap(*results))\n",
    "\n",
    "t4 = time.time()\n",
    "\n",
    "print(t4-t3)\n",
    "\n",
    "storeData(final_result,'testoutput-unlabelled.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
