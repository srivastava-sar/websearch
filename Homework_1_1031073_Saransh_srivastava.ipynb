{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Preprocessing and Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Saransh Srivastava\n",
    "\n",
    "Student ID: 1031073"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Due date</b>: Friday, 29 Mar 2019 4pm\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day (both week and weekend days counted)\n",
    "\n",
    "<b>Marks</b>: 6% of mark for class (with 5% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll be using documents from a Wall Street Journal text corpus to create a space efficient inverted index capable of fast TF-IDF query processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we will be using documents from a Wall Street Journal text corpus. The corpus can be downloaded with the commands below. Each line contains <i>one</i> document which you should tokenize and stem using tools provided by NLTK. Some of the steps below are already provided whereas others have to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to download documents from a Wall Street Journal text corpus. <b><i>No implementation is needed.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'wsta_col_20k.gz'\n",
    "my_file = Path(fname)\n",
    "if not my_file.is_file():\n",
    "    url = 'https://trevorcohn.github.io/comp90042/resources/' + fname\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Save to the current directory\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to read raw documents, one document per line. <b><i>No implementation is needed.</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "John Blair & Co. is close to an agreement to sell its TV station advertising representation operation and program production unit to an investor group led by James H. Rosenfield, a former CBS Inc. executive, industry sources said. Industry sources put the value of the proposed acquisition at more than $100 million. John Blair was acquired last year by Reliance Capital Group Inc., which has been divesting itself of John Blair's major assets. John Blair represents about 130 local television stations in the placement of national and other advertising. Mr. Rosenfield stepped down as a senior executive vice president of CBS Broadcasting in December 1985 under a CBS early retirement program. Neither Mr. Rosenfield nor officials of John Blair could be reached for comment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "raw_docs = []\n",
    "with gzip.open(fname, 'rt') as f:\n",
    "    for raw_doc in f:\n",
    "        raw_docs.append(raw_doc)\n",
    "\n",
    "print(len(raw_docs))\n",
    "print(raw_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Here, you will do preprocessing. You should *tokenize* each document, *stem* and *lowercase* each token using NLTK `word_tokenize` and `PorterStemmer`, and create a *vocabulary* for all the terms (normalized types). Each term should be assigned a unique ID. Note that we are not doing any stop word removal. The vocabulary should be built as a Python *map*, mapping from all the terms $M$ to their term IDs (integers of $[0..M-1]$). The processing may take a few minutes. \n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected output.\n",
    "    \n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 20000\n",
      "Number of unique terms = 103193\n",
      "Number of tokens = 9140697\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "###\n",
    "# Please include the following import lines if word_tokenize is not downloaded due to 'punkt' not installede\n",
    "#nltk.download('punkt')\n",
    "###\n",
    "\n",
    "\n",
    "# processed_docs stores the list of processed docs\n",
    "processed_docs = []\n",
    "# vocab contains (term, term id) pairs\n",
    "vocab = {}\n",
    "# total_tokens stores the total number of tokens\n",
    "total_tokens = 0\n",
    "total_terms = 0\n",
    "\n",
    "# TODO: iterate over docs, tokenize, stem and add to vocab and assign ID if new token\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for raw_doc in raw_docs:\n",
    "    \n",
    "    # norm_doc stores the normalized tokens of a doc\n",
    "    norm_doc = []\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    token_doc = word_tokenize(raw_doc)\n",
    "    for token_word in token_doc:\n",
    "        total_tokens += 1\n",
    "        # total tokens are also representative of IDs of the terms\n",
    "        term = stemmer.stem(token_word).lower()\n",
    "        norm_doc.append(term)\n",
    "        if term not in vocab:\n",
    "            vocab[term] = total_terms\n",
    "            total_terms += 1\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    processed_docs.append(norm_doc)\n",
    "\n",
    "    \n",
    "print(\"Number of documents = {}\".format(len(processed_docs)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "print(\"Number of tokens = {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(processed_docs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(vocab) > 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you should build a Python `Counter` to count the term frequencies of each document. For each document, the counter should map the terms to term frequencies. All the counters (for all documents) should be stored in a list called *doc_term_freqs*.\n",
    "\n",
    "For example, here is a document:\n",
    "\n",
    ">the old night keeper keeps the keep in the town. in the big old house in the big old gown. The house in the town had the big old keep where the old night keeper never did sleep. The keeper keeps the keep in the night and keeps in the dark and sleeps in the light.\n",
    "\n",
    "After the tokenisation and stemming, a counter as below should be built for the document:\n",
    "\n",
    "`\n",
    "Counter({'the': 14, 'in': 7, 'keep': 6, 'old': 5, '.': 4, 'night': 3, 'keeper': 3, 'big': 3, 'town': 2, 'hous': 2, 'sleep': 2, 'and': 2, 'gown': 1, 'had': 1, 'where': 1, 'never': 1, 'did': 1, 'dark': 1, 'light': 1})\n",
    "`\n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected *doc_term_freqs*.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# doc_term_freqs stores the counters (mapping terms to term frequencies) of all documents\n",
    "doc_term_freqs = []\n",
    "\n",
    "# TODO iterate over document and for each document produce the term frequency map and store in the list\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "for proc_doc in processed_docs:\n",
    "    doc_term_freqs.append(Counter(proc_doc))\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(len(doc_term_freqs))\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(doc_term_freqs) == 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[0][\"blair\"] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(doc_term_freqs[100][\"bank\"] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inverted Index (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Run the code below to create an `InvertedIndex` class using `vocab` and `doc_term_freqs` that you built earlier. <b><i>No implementation is needed.</i></b>\n",
    "\n",
    "Our `InvertedIndex` class contains <b>six</b> components:\n",
    "\n",
    "1. The vocabulary `vocab`, which will be used to map query terms to term ids\n",
    "2. The length of each document,  `doc_len`\n",
    "3. `doc_ids` is a list indexed by term IDs. For each term ID, it stores a list of document ids of all documents containing that term\n",
    "4. `doc_term_freqs` is a list indexed by term IDs. For each term ID, it stores a list of document term frequencies $f_{d,t}$ (how often a document $d$ contains the term $t$) of the corresponding documents stored in `doc_ids`\n",
    "5. `doc_freqs` is a list indexed by term IDs. For each term ID, it stores the document frequency $f_t$ indicating the number of documents containing one or more occurrences of term $t$;\n",
    "6. Two integers `total_num_docs` and `max_doc_len` store the total number of documents and the maximum document length\n",
    "\n",
    "These values will be used in the code below. Note that some of these components are for display purposes to verify that your implementation was correctly processing the text collection, but won't be used in TF-IDF scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes each integer is stored using 8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "    \n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "print(\"longest document length = {}\".format(invindex.max_doc_len))\n",
    "print(\"uncompressed space usage MiB = {:.3f}\".format(invindex.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you will use the `InvertedIndex` class to compute the TF-IDF similarity scores for the documents given a simple query $Q$.\n",
    "\n",
    "Here is a simplified formula for computing TF-IDF similarity scores:\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}\n",
    "\n",
    "where $Q$ corresponds to a query containing $q$ query terms, $\\sqrt{|d|}$ corresponds to the length of the document (in words), $f_{d,t}$ corresponds to the frequency of term $t$ in document $d$, $N$ corresponds to the number of documents in the collection, and $f_t$ corresponds to the document frequency of term $t$. All these information are available in the `InvertedIndex` class. Note that the formulation of TF-IDF is a little different to the formula for TF-IDF shown in the lectures. We have adapted the formulation here to allow for a simpler implementation, e.g., avoiding the need for repeated passes over the dataset. (All manner of variants of TF-IDF exist in practise.)\n",
    "\n",
    "You should implement the `query_tfidf` function. The `query_tfidf` function should take a query and an inverted index and output the top $k$ highest scoring documents. \n",
    "\n",
    "For example, here is a query.\n",
    "\n",
    "> south korea production\n",
    "\n",
    "Here is a sample result.\n",
    "\n",
    "> RANK  1  DOCID  176  SCORE  0.426  CONTENT  South Korea rose 1% in February from a year earlier, the\n",
    "> \n",
    "> ...\n",
    "\n",
    "You may check the section, <i>\"For your testing\"</i>, below for the expected output.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# given a query and an index returns a list of the k highest scoring documents as tuples containing <docid,score>\n",
    "def query_tfidf(query, index, k=10):\n",
    "    \n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    # It is assumed that if the query terms are not availab le in the vocabulary\n",
    "    # then the search engine does not return back any document.\n",
    "    # Also, if the search query ranks less than k documents in the filtering then only those documents are returned\n",
    "    \n",
    "    for q in query:\n",
    "        if q not in vocab:\n",
    "            continue\n",
    "        for doc_id_idx,doc_id in enumerate(index.docids(q)):\n",
    "            fdt_log = log(1 + index.freqs(q)[doc_id_idx])\n",
    "            ft_log = log(index.total_num_docs / index.f_t(q))\n",
    "            scores[doc_id] += (fdt_log * ft_log) / sqrt(index.doc_len[doc_id])\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return scores.most_common(k)\n",
    "\n",
    "\n",
    "# We output some statistics from our index\n",
    "query = \"south korea productions\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "results = query_tfidf(stemmed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    # e.g RANK 1 DOCID 176 SCORE 0.426 CONTENT South Korea rose 1% in February from a year earlier, the\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: DOCID\n",
    "assert(results[0][0] > 500 and results[0][0] < 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 1: SCORE\n",
    "assert(results[0][1] > 0.5 and results[0][1] < 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vbyte compression and decompression (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will reduce the space usage of the inverted index by compression. We will <i>compress</i> the `doc_ids` and `doc_term_freqs` lists in the inverted index using <b>vbyte</b> compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: You should implement two methods to perform vbyte compression and decompression as described in the lecture slides. The method signatures are provided below. \n",
    "\n",
    "- The first method `vbyte_encode(num)` should receive a number as an integer and produces a list of output bytes encoding the number. \n",
    "- The second method `vbyte_decode(input_bytes, idx)` should receive a list of input bytes and an offset into that list where the decompression should start. It returns the decoded number and the number of bytes consumed to decode the number.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vbyte_encode(num):\n",
    "\n",
    "    # out_bytes stores a list of output bytes encoding the number\n",
    "    out_bytes = []\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    while num >= 128:\n",
    "        out_bytes.append(num %128)\n",
    "        num = num >> 7\n",
    "    out_bytes.append(num+128)\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return out_bytes\n",
    "\n",
    "\n",
    "def vbyte_decode(input_bytes, idx):\n",
    "    \n",
    "    # x stores the decoded number\n",
    "    x = 0\n",
    "    # consumed stores the number of bytes consumed to decode the number\n",
    "    consumed = 0\n",
    "\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    shift = 0\n",
    "    decode_num = input_bytes[idx]\n",
    "    while decode_num < 128:\n",
    "        idx += 1\n",
    "        x = x ^ (decode_num << shift)\n",
    "        shift += 7\n",
    "        decode_num = input_bytes[idx]\n",
    "        consumed += 1\n",
    "    x = x ^ ((decode_num-128) << shift)\n",
    "    consumed += 1\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    \n",
    "    return x, consumed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a sanity check, we ensure that compression and decompression work correctly:\n",
    "for num in range(0, 123456):\n",
    "    vb = vbyte_encode(num)\n",
    "    dec, decoded_bytes = vbyte_decode(vb, 0)\n",
    "    assert(num == dec)\n",
    "    assert(decoded_bytes == len(vb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, you should modify the `InvertedIndex` class to support compression.\n",
    "\n",
    "Your task here is to implement the compression of the `doc_ids` and `doc_term_freqs` lists using the `vbyte_encode` function implemented earlier. Note that the `doc_ids` have to be gap encoded as described in the lecture slides. A helper function `decompress_list` is provided to allow easy decompression of the lists. \n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_list(input_bytes, gapped_encoded):\n",
    "    res = []\n",
    "    prev = 0\n",
    "    idx = 0\n",
    "    while idx < len(input_bytes):\n",
    "        dec_num, consumed_bytes = vbyte_decode(input_bytes, idx)\n",
    "        idx += consumed_bytes\n",
    "        num = dec_num + prev\n",
    "        res.append(num)\n",
    "        if gapped_encoded:\n",
    "            prev = num\n",
    "    return res\n",
    "\n",
    "class CompressedInvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "        # TODO NOW WE COMPRESS THE LISTS\n",
    "        \n",
    "        ###\n",
    "        # Your answer BEGINS HERE\n",
    "        ###\n",
    "        for doc_termId_Idx,doc_termId in enumerate(self.doc_ids):\n",
    "            prev = 0\n",
    "            docId_list = []\n",
    "            for docId in doc_termId:\n",
    "                for id in vbyte_encode(docId - prev):\n",
    "                    docId_list.append(id)\n",
    "                prev = docId\n",
    "            self.doc_ids[doc_termId_Idx] = docId_list\n",
    "        \n",
    "        for doc_termId_idx,doc_termId in enumerate(self.doc_term_freqs):\n",
    "            term_freq_list = []\n",
    "            for docTF in doc_termId:\n",
    "                for freq in vbyte_encode(docTF):\n",
    "                    term_freq_list.append(freq)\n",
    "            self.doc_term_freqs[doc_termId_idx] = term_freq_list\n",
    "        ###\n",
    "        # Your answer ENDS HERE\n",
    "        ###\n",
    "    \n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_ids[term_id], True)\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # We decompress\n",
    "        return decompress_list(self.doc_term_freqs[term_id], False)\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes the integers are now bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list)\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list)\n",
    "        return space_usage\n",
    "\n",
    "\n",
    "# We output the same statistics as before to ensure we still store the same data but now use much less space\n",
    "compressed_index = CompressedInvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "print(\"documents = {}\".format(compressed_index.num_docs()))\n",
    "print(\"unique terms = {}\".format(compressed_index.num_terms()))\n",
    "print(\"longest document = {}\".format(compressed_index.max_doc_len))\n",
    "print(\"compressed space usage MiB = {:.3f}\".format(compressed_index.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additionally we want to ensure that the index still returns the same results as before\n",
    "query = \"south korea productions\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "comp_results = query_tfidf(stemmed_query, compressed_index)\n",
    "for rank, res in enumerate(comp_results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
